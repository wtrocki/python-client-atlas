# coding: utf-8

"""
    MongoDB Atlas Administration API

    The MongoDB Atlas Administration API allows developers to manage all components in MongoDB Atlas. To learn more, review the [Administration API overview](https://www.mongodb.com/docs/atlas/api/atlas-admin-api/). This OpenAPI specification covers all of the collections with the exception of Alerts, Alert Configurations, and Events. Refer to the [legacy documentation](https://www.mongodb.com/docs/atlas/reference/api-resources/) for the specifications of these resources.  # noqa: E501

    The version of the OpenAPI document: 2.0
    Generated by: https://openapi-generator.tech
"""


from __future__ import annotations
from inspect import getfullargspec
import pprint
import re  # noqa: F401
import json


from typing import List, Optional
from pydantic import BaseModel, Field, StrictStr, constr, validator
from atlas.models.api_atlas_fts_analyzers_view_manual import ApiAtlasFTSAnalyzersViewManual
from atlas.models.api_atlas_fts_mappings_view_manual import ApiAtlasFTSMappingsViewManual
from atlas.models.fts_synonym_mapping_definition import FTSSynonymMappingDefinition

class FTSIndex(BaseModel):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """
    analyzer: Optional[StrictStr] = Field('lucene.standard', description="Specific pre-defined method chosen to convert database field text into searchable words. This conversion reduces the text of fields into the smallest units of text. These units are called a **term** or **token**. This process, known as tokenization, involves a variety of changes made to the text in fields:  - extracting words - removing punctuation - removing accents - changing to lowercase - removing common words - reducing words to their root form (stemming) - changing words to their base form (lemmatization)  MongoDB Cloud uses the selected process to build the Atlas Search index.")
    analyzers: Optional[List[ApiAtlasFTSAnalyzersViewManual]] = Field(None, description="List of user-defined methods to convert database field text into searchable words.")
    collection_name: StrictStr = Field(..., alias="collectionName", description="Human-readable label that identifies the collection that contains one or more Atlas Search indexes.")
    database: StrictStr = Field(..., description="Human-readable label that identifies the database that contains the collection with one or more Atlas Search indexes.")
    index_id: Optional[constr(strict=True, max_length=24, min_length=24)] = Field(None, alias="indexID", description="Unique 24-hexadecimal digit string that identifies this Atlas Search index.")
    mappings: Optional[ApiAtlasFTSMappingsViewManual] = None
    name: StrictStr = Field(..., description="Human-readable label that identifies this index. Within each namespace, names of all indexes in the namespace must be unique.")
    search_analyzer: Optional[StrictStr] = Field('lucene.standard', alias="searchAnalyzer", description="Method applied to identify words when searching this index.")
    status: Optional[StrictStr] = Field(None, description="Condition of the search index when you made this request.  | Status | Index Condition |  |---|---|  | IN_PROGRESS | Atlas is building or re-building the index after an edit. |  | STEADY | You can use this search index. |  | FAILED | Atlas could not build the index. |  | MIGRATING | Atlas is upgrading the underlying cluster tier and migrating indexes. |  | PAUSED | The cluster is paused. | ")
    synonyms: Optional[List[FTSSynonymMappingDefinition]] = Field(None, description="Rule sets that map words to their synonyms in this index.")
    __properties = ["analyzer", "analyzers", "collectionName", "database", "indexID", "mappings", "name", "searchAnalyzer", "status", "synonyms"]

    @validator('analyzer')
    def analyzer_validate_enum(cls, v):
        if v is None:
            return v

        if v not in ('lucene.standard', 'lucene.simple', 'lucene.whitespace', 'lucene.keyword', 'lucene.arabic', 'lucene.armenian', 'lucene.basque', 'lucene.bengali', 'lucene.brazilian', 'lucene.bulgarian', 'lucene.catalan', 'lucene.chinese', 'lucene.cjk', 'lucene.czech', 'lucene.danish', 'lucene.dutch', 'lucene.english', 'lucene.finnish', 'lucene.french', 'lucene.galician', 'lucene.german', 'lucene.greek', 'lucene.hindi', 'lucene.hungarian', 'lucene.indonesian', 'lucene.irish', 'lucene.italian', 'lucene.japanese', 'lucene.korean', 'lucene.kuromoji', 'lucene.latvian', 'lucene.lithuanian', 'lucene.morfologik', 'lucene.nori', 'lucene.norwegian', 'lucene.persian', 'lucene.portuguese', 'lucene.romanian', 'lucene.russian', 'lucene.smartcn', 'lucene.sorani', 'lucene.spanish', 'lucene.swedish', 'lucene.thai', 'lucene.turkish', 'lucene.ukrainian'):
            raise ValueError("must validate the enum values ('lucene.standard', 'lucene.simple', 'lucene.whitespace', 'lucene.keyword', 'lucene.arabic', 'lucene.armenian', 'lucene.basque', 'lucene.bengali', 'lucene.brazilian', 'lucene.bulgarian', 'lucene.catalan', 'lucene.chinese', 'lucene.cjk', 'lucene.czech', 'lucene.danish', 'lucene.dutch', 'lucene.english', 'lucene.finnish', 'lucene.french', 'lucene.galician', 'lucene.german', 'lucene.greek', 'lucene.hindi', 'lucene.hungarian', 'lucene.indonesian', 'lucene.irish', 'lucene.italian', 'lucene.japanese', 'lucene.korean', 'lucene.kuromoji', 'lucene.latvian', 'lucene.lithuanian', 'lucene.morfologik', 'lucene.nori', 'lucene.norwegian', 'lucene.persian', 'lucene.portuguese', 'lucene.romanian', 'lucene.russian', 'lucene.smartcn', 'lucene.sorani', 'lucene.spanish', 'lucene.swedish', 'lucene.thai', 'lucene.turkish', 'lucene.ukrainian')")
        return v

    @validator('index_id')
    def index_id_validate_regular_expression(cls, v):
        if not re.match(r"^([a-f0-9]{24})$", v):
            raise ValueError(r"must validate the regular expression /^([a-f0-9]{24})$/")
        return v

    @validator('search_analyzer')
    def search_analyzer_validate_enum(cls, v):
        if v is None:
            return v

        if v not in ('lucene.standard', 'lucene.simple', 'lucene.whitespace', 'lucene.keyword', 'lucene.arabic', 'lucene.armenian', 'lucene.basque', 'lucene.bengali', 'lucene.brazilian', 'lucene.bulgarian', 'lucene.catalan', 'lucene.chinese', 'lucene.cjk', 'lucene.czech', 'lucene.danish', 'lucene.dutch', 'lucene.english', 'lucene.finnish', 'lucene.french', 'lucene.galician', 'lucene.german', 'lucene.greek', 'lucene.hindi', 'lucene.hungarian', 'lucene.indonesian', 'lucene.irish', 'lucene.italian', 'lucene.japanese', 'lucene.korean', 'lucene.kuromoji', 'lucene.latvian', 'lucene.lithuanian', 'lucene.morfologik', 'lucene.nori', 'lucene.norwegian', 'lucene.persian', 'lucene.portuguese', 'lucene.romanian', 'lucene.russian', 'lucene.smartcn', 'lucene.sorani', 'lucene.spanish', 'lucene.swedish', 'lucene.thai', 'lucene.turkish', 'lucene.ukrainian'):
            raise ValueError("must validate the enum values ('lucene.standard', 'lucene.simple', 'lucene.whitespace', 'lucene.keyword', 'lucene.arabic', 'lucene.armenian', 'lucene.basque', 'lucene.bengali', 'lucene.brazilian', 'lucene.bulgarian', 'lucene.catalan', 'lucene.chinese', 'lucene.cjk', 'lucene.czech', 'lucene.danish', 'lucene.dutch', 'lucene.english', 'lucene.finnish', 'lucene.french', 'lucene.galician', 'lucene.german', 'lucene.greek', 'lucene.hindi', 'lucene.hungarian', 'lucene.indonesian', 'lucene.irish', 'lucene.italian', 'lucene.japanese', 'lucene.korean', 'lucene.kuromoji', 'lucene.latvian', 'lucene.lithuanian', 'lucene.morfologik', 'lucene.nori', 'lucene.norwegian', 'lucene.persian', 'lucene.portuguese', 'lucene.romanian', 'lucene.russian', 'lucene.smartcn', 'lucene.sorani', 'lucene.spanish', 'lucene.swedish', 'lucene.thai', 'lucene.turkish', 'lucene.ukrainian')")
        return v

    @validator('status')
    def status_validate_enum(cls, v):
        if v is None:
            return v

        if v not in ('IN_PROGRESS', 'STEADY', 'FAILED', 'MIGRATING', 'STALE', 'PAUSED'):
            raise ValueError("must validate the enum values ('IN_PROGRESS', 'STEADY', 'FAILED', 'MIGRATING', 'STALE', 'PAUSED')")
        return v

    class Config:
        allow_population_by_field_name = True
        validate_assignment = True

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.dict(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> FTSIndex:
        """Create an instance of FTSIndex from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self):
        """Returns the dictionary representation of the model using alias"""
        _dict = self.dict(by_alias=True,
                          exclude={
                            "index_id",
                            "status",
                          },
                          exclude_none=True)
        # override the default output from pydantic by calling `to_dict()` of each item in analyzers (list)
        _items = []
        if self.analyzers:
            for _item in self.analyzers:
                if _item:
                    _items.append(_item.to_dict())
            _dict['analyzers'] = _items
        # override the default output from pydantic by calling `to_dict()` of mappings
        if self.mappings:
            _dict['mappings'] = self.mappings.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in synonyms (list)
        _items = []
        if self.synonyms:
            for _item in self.synonyms:
                if _item:
                    _items.append(_item.to_dict())
            _dict['synonyms'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: dict) -> FTSIndex:
        """Create an instance of FTSIndex from a dict"""
        if obj is None:
            return None

        if type(obj) is not dict:
            return FTSIndex.parse_obj(obj)

        _obj = FTSIndex.parse_obj({
            "analyzer": obj.get("analyzer") if obj.get("analyzer") is not None else 'lucene.standard',
            "analyzers": [ApiAtlasFTSAnalyzersViewManual.from_dict(_item) for _item in obj.get("analyzers")] if obj.get("analyzers") is not None else None,
            "collection_name": obj.get("collectionName"),
            "database": obj.get("database"),
            "index_id": obj.get("indexID"),
            "mappings": ApiAtlasFTSMappingsViewManual.from_dict(obj.get("mappings")) if obj.get("mappings") is not None else None,
            "name": obj.get("name"),
            "search_analyzer": obj.get("searchAnalyzer") if obj.get("searchAnalyzer") is not None else 'lucene.standard',
            "status": obj.get("status"),
            "synonyms": [FTSSynonymMappingDefinition.from_dict(_item) for _item in obj.get("synonyms")] if obj.get("synonyms") is not None else None
        })
        return _obj

