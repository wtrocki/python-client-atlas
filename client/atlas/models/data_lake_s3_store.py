# coding: utf-8

"""
    MongoDB Atlas Administration API

    The MongoDB Atlas Administration API allows developers to manage all components in MongoDB Atlas. To learn more, review the [Administration API overview](https://www.mongodb.com/docs/atlas/api/atlas-admin-api/). This OpenAPI specification covers all of the collections with the exception of Alerts, Alert Configurations, and Events. Refer to the [legacy documentation](https://www.mongodb.com/docs/atlas/reference/api-resources/) for the specifications of these resources.  # noqa: E501

    The version of the OpenAPI document: 2.0
    Generated by: https://openapi-generator.tech
"""


from __future__ import annotations
from inspect import getfullargspec
import pprint
import re  # noqa: F401
import json


from typing import List, Optional
from pydantic import BaseModel, Field, StrictBool, StrictStr, validator

class DataLakeS3Store(BaseModel):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """
    additional_storage_classes: Optional[List[StrictStr]] = Field(None, alias="additionalStorageClasses", description="Collection of AWS S3 [storage classes](https://aws.amazon.com/s3/storage-classes/). Atlas Data Lake includes the files in these storage classes in the query results.")
    bucket: Optional[StrictStr] = Field(None, description="Human-readable label that identifies the AWS S3 bucket. This label must exactly match the name of an S3 bucket that the data lake can access with the configured AWS Identity and Access Management (IAM) credentials.")
    delimiter: Optional[StrictStr] = Field(None, description="The delimiter that separates **databases.[n].collections.[n].dataSources.[n].path** segments in the data store. MongoDB Cloud uses the delimiter to efficiently traverse S3 buckets with a hierarchical directory structure. You can specify any character supported by the S3 object keys as the delimiter. For example, you can specify an underscore (_) or a plus sign (+) or multiple characters, such as double underscores (__) as the delimiter. If omitted, defaults to `/`.")
    include_tags: Optional[StrictBool] = Field(False, alias="includeTags", description="Flag that indicates whether to use S3 tags on the files in the given path as additional partition attributes. If set to `true`, data lake adds the S3 tags as additional partition attributes and adds new top-level BSON elements associating each tag to each document.")
    prefix: Optional[StrictStr] = Field(None, description="Prefix that MongoDB Cloud applies when searching for files in the S3 bucket. The data store prepends the value of prefix to the **databases.[n].collections.[n].dataSources.[n].path** to create the full path for files to ingest. If omitted, MongoDB Cloud searches all files from the root of the S3 bucket.")
    public: Optional[StrictBool] = Field(False, description="Flag that indicates whether the bucket is public. If set to `true`, MongoDB Cloud doesn't use the configured AWS Identity and Access Management (IAM) role to access the S3 bucket. If set to `false`, the configured AWS IAM role must include permissions to access the S3 bucket.")
    region: Optional[StrictStr] = Field(None, description=" Physical location where MongoDB Cloud deploys your AWS-hosted MongoDB cluster nodes. The region you choose can affect network latency for clients accessing your databases. When MongoDB Cloud deploys a dedicated cluster, it checks if a VPC or VPC connection exists for that provider and region. If not, MongoDB Cloud creates them as part of the deployment. MongoDB Cloud assigns the VPC a CIDR block. To limit a new VPC peering connection to one CIDR block and region, create the connection first. Deploy the cluster after the connection starts.")
    name: Optional[StrictStr] = Field(None, description="Human-readable label that identifies the data store. The **databases.[n].collections.[n].dataSources.[n].storeName** field references this values as part of the mapping configuration. To use MongoDB Cloud as a data store, the data lake requires a serverless instance or an `M10` or higher cluster.")
    provider: StrictStr = ...
    __properties = ["additionalStorageClasses", "bucket", "delimiter", "includeTags", "prefix", "public", "region", "name", "provider"]

    @validator('additional_storage_classes')
    def additional_storage_classes_validate_enum(cls, v):
        if v is None:
            return v

        if v not in ('STANDARD', 'INTELLIGENT_TIERING', 'STANDARD_IA'):
            raise ValueError("must validate the enum values ('STANDARD', 'INTELLIGENT_TIERING', 'STANDARD_IA')")
        return v

    @validator('region')
    def region_validate_enum(cls, v):
        if v is None:
            return v

        if v not in ('US_GOV_WEST_1', 'US_GOV_EAST_1', 'US_EAST_1', 'US_EAST_2', 'US_WEST_1', 'US_WEST_2', 'CA_CENTRAL_1', 'EU_NORTH_1', 'EU_WEST_1', 'EU_WEST_2', 'EU_WEST_3', 'EU_CENTRAL_1', 'AP_EAST_1', 'AP_NORTHEAST_1', 'AP_NORTHEAST_2', 'AP_NORTHEAST_3', 'AP_SOUTHEAST_1', 'AP_SOUTHEAST_2', 'AP_SOUTHEAST_3', 'AP_SOUTH_1', 'SA_EAST_1', 'CN_NORTH_1', 'CN_NORTHWEST_1', 'ME_SOUTH_1', 'AF_SOUTH_1', 'EU_SOUTH_1', 'GLOBAL'):
            raise ValueError("must validate the enum values ('US_GOV_WEST_1', 'US_GOV_EAST_1', 'US_EAST_1', 'US_EAST_2', 'US_WEST_1', 'US_WEST_2', 'CA_CENTRAL_1', 'EU_NORTH_1', 'EU_WEST_1', 'EU_WEST_2', 'EU_WEST_3', 'EU_CENTRAL_1', 'AP_EAST_1', 'AP_NORTHEAST_1', 'AP_NORTHEAST_2', 'AP_NORTHEAST_3', 'AP_SOUTHEAST_1', 'AP_SOUTHEAST_2', 'AP_SOUTHEAST_3', 'AP_SOUTH_1', 'SA_EAST_1', 'CN_NORTH_1', 'CN_NORTHWEST_1', 'ME_SOUTH_1', 'AF_SOUTH_1', 'EU_SOUTH_1', 'GLOBAL')")
        return v

    class Config:
        allow_population_by_field_name = True
        validate_assignment = True

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.dict(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> DataLakeS3Store:
        """Create an instance of DataLakeS3Store from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self):
        """Returns the dictionary representation of the model using alias"""
        _dict = self.dict(by_alias=True,
                          exclude={
                          },
                          exclude_none=True)
        return _dict

    @classmethod
    def from_dict(cls, obj: dict) -> DataLakeS3Store:
        """Create an instance of DataLakeS3Store from a dict"""
        if obj is None:
            return None

        if type(obj) is not dict:
            return DataLakeS3Store.parse_obj(obj)

        _obj = DataLakeS3Store.parse_obj({
            "additional_storage_classes": obj.get("additionalStorageClasses"),
            "bucket": obj.get("bucket"),
            "delimiter": obj.get("delimiter"),
            "include_tags": obj.get("includeTags") if obj.get("includeTags") is not None else False,
            "prefix": obj.get("prefix"),
            "public": obj.get("public") if obj.get("public") is not None else False,
            "region": obj.get("region"),
            "name": obj.get("name"),
            "provider": obj.get("provider")
        })
        return _obj

