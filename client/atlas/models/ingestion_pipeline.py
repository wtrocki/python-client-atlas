# coding: utf-8

"""
    MongoDB Atlas Administration API

    The MongoDB Atlas Administration API allows developers to manage all components in MongoDB Atlas. To learn more, review the [Administration API overview](https://www.mongodb.com/docs/atlas/api/atlas-admin-api/). This OpenAPI specification covers all of the collections with the exception of Alerts, Alert Configurations, and Events. Refer to the [legacy documentation](https://www.mongodb.com/docs/atlas/reference/api-resources/) for the specifications of these resources.  # noqa: E501

    The version of the OpenAPI document: 2.0
    Generated by: https://openapi-generator.tech
"""


from __future__ import annotations
from inspect import getfullargspec
import pprint
import re  # noqa: F401
import json

from datetime import datetime
from typing import List, Optional
from pydantic import BaseModel, Field, StrictStr, constr, validator
from atlas.models.field_transformation import FieldTransformation
from atlas.models.ingestion_sink import IngestionSink
from atlas.models.ingestion_source import IngestionSource

class IngestionPipeline(BaseModel):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """
    id: Optional[constr(strict=True, max_length=24, min_length=24)] = Field(None, alias="_id", description="Unique 24-hexadecimal digit string that identifies the Data Lake Pipeline.")
    created_date: Optional[datetime] = Field(None, alias="createdDate", description="Timestamp that indicates when the Data Lake Pipeline was created.")
    group_id: Optional[constr(strict=True, max_length=24, min_length=24)] = Field(None, alias="groupId", description="Unique 24-hexadecimal digit string that identifies the group.")
    last_updated_date: Optional[datetime] = Field(None, alias="lastUpdatedDate", description="Timestamp that indicates the last time that the Data Lake Pipeline was updated.")
    name: Optional[StrictStr] = Field(None, description="Name of this Data Lake Pipeline.")
    sink: Optional[IngestionSink] = None
    source: Optional[IngestionSource] = None
    state: Optional[StrictStr] = Field(None, description="State of this Data Lake Pipeline.")
    transformations: Optional[List[FieldTransformation]] = Field(None, description="Fields to be excluded for this Data Lake Pipeline.")
    __properties = ["_id", "createdDate", "groupId", "lastUpdatedDate", "name", "sink", "source", "state", "transformations"]

    @validator('id')
    def id_validate_regular_expression(cls, v):
        if not re.match(r"^([a-f0-9]{24})$", v):
            raise ValueError(r"must validate the regular expression /^([a-f0-9]{24})$/")
        return v

    @validator('group_id')
    def group_id_validate_regular_expression(cls, v):
        if not re.match(r"^([a-f0-9]{24})$", v):
            raise ValueError(r"must validate the regular expression /^([a-f0-9]{24})$/")
        return v

    @validator('state')
    def state_validate_enum(cls, v):
        if v is None:
            return v

        if v not in ('ACTIVE', 'PAUSED'):
            raise ValueError("must validate the enum values ('ACTIVE', 'PAUSED')")
        return v

    class Config:
        allow_population_by_field_name = True
        validate_assignment = True

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.dict(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> IngestionPipeline:
        """Create an instance of IngestionPipeline from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self):
        """Returns the dictionary representation of the model using alias"""
        _dict = self.dict(by_alias=True,
                          exclude={
                            "id",
                            "created_date",
                            "group_id",
                            "last_updated_date",
                            "state",
                          },
                          exclude_none=True)
        # override the default output from pydantic by calling `to_dict()` of sink
        if self.sink:
            _dict['sink'] = self.sink.to_dict()
        # override the default output from pydantic by calling `to_dict()` of source
        if self.source:
            _dict['source'] = self.source.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in transformations (list)
        _items = []
        if self.transformations:
            for _item in self.transformations:
                if _item:
                    _items.append(_item.to_dict())
            _dict['transformations'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: dict) -> IngestionPipeline:
        """Create an instance of IngestionPipeline from a dict"""
        if obj is None:
            return None

        if type(obj) is not dict:
            return IngestionPipeline.parse_obj(obj)

        _obj = IngestionPipeline.parse_obj({
            "id": obj.get("_id"),
            "created_date": obj.get("createdDate"),
            "group_id": obj.get("groupId"),
            "last_updated_date": obj.get("lastUpdatedDate"),
            "name": obj.get("name"),
            "sink": IngestionSink.from_dict(obj.get("sink")) if obj.get("sink") is not None else None,
            "source": IngestionSource.from_dict(obj.get("source")) if obj.get("source") is not None else None,
            "state": obj.get("state"),
            "transformations": [FieldTransformation.from_dict(_item) for _item in obj.get("transformations")] if obj.get("transformations") is not None else None
        })
        return _obj

