# coding: utf-8

"""
    MongoDB Atlas Administration API

    The MongoDB Atlas Administration API allows developers to manage all components in MongoDB Atlas. To learn more, review the [Administration API overview](https://www.mongodb.com/docs/atlas/api/atlas-admin-api/). This OpenAPI specification covers all of the collections with the exception of Alerts, Alert Configurations, and Events. Refer to the [legacy documentation](https://www.mongodb.com/docs/atlas/reference/api-resources/) for the specifications of these resources.  # noqa: E501

    The version of the OpenAPI document: 2.0
    Generated by: https://openapi-generator.tech
"""

from datetime import date, datetime  # noqa: F401
import decimal  # noqa: F401
import functools  # noqa: F401
import io  # noqa: F401
import re  # noqa: F401
import typing  # noqa: F401
import typing_extensions  # noqa: F401
import uuid  # noqa: F401

import frozendict  # noqa: F401

from atlas import schemas  # noqa: F401


class ApiAtlasFTSAnalyzersViewManual(
    schemas.DictSchema
):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.

    Settings that describe one Atlas Search custom analyzer.
    """


    class MetaOapg:
        required = {
            "name",
            "tokenizer",
        }
        
        class properties:
            name = schemas.StrSchema
            
            
            class tokenizer(
                schemas.ComposedBase,
                schemas.DictSchema
            ):
            
            
                class MetaOapg:
                    
                    @classmethod
                    @functools.lru_cache()
                    def one_of(cls):
                        # we need this here to make our import statements work
                        # we must store _composed_schemas in here so the code is only run
                        # when we invoke this method. If we kept this at the class
                        # level we would get an error because the class level
                        # code would be run when this module is imported, and these composed
                        # classes don't exist yet because their module has not finished
                        # loading
                        return [
                            TokenizeredgeGram,
                            Tokenizerkeyword,
                            TokenizernGram,
                            TokenizerregexCaptureGroup,
                            TokenizerregexSplit,
                            Tokenizerstandard,
                            TokenizeruaxUrlEmail,
                            Tokenizerwhitespace,
                        ]
            
            
                def __new__(
                    cls,
                    *_args: typing.Union[dict, frozendict.frozendict, ],
                    _configuration: typing.Optional[schemas.Configuration] = None,
                    **kwargs: typing.Union[schemas.AnyTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, None, list, tuple, bytes],
                ) -> 'tokenizer':
                    return super().__new__(
                        cls,
                        *_args,
                        _configuration=_configuration,
                        **kwargs,
                    )
            
            
            class charFilters(
                schemas.ListSchema
            ):
            
            
                class MetaOapg:
                    
                    
                    class items(
                        schemas.ComposedBase,
                        schemas.DictSchema
                    ):
                    
                    
                        class MetaOapg:
                            
                            @classmethod
                            @functools.lru_cache()
                            def one_of(cls):
                                # we need this here to make our import statements work
                                # we must store _composed_schemas in here so the code is only run
                                # when we invoke this method. If we kept this at the class
                                # level we would get an error because the class level
                                # code would be run when this module is imported, and these composed
                                # classes don't exist yet because their module has not finished
                                # loading
                                return [
                                    CharFilterhtmlStrip,
                                    CharFiltericuNormalize,
                                    CharFiltermapping,
                                    CharFilterpersian,
                                ]
                    
                    
                        def __new__(
                            cls,
                            *_args: typing.Union[dict, frozendict.frozendict, ],
                            _configuration: typing.Optional[schemas.Configuration] = None,
                            **kwargs: typing.Union[schemas.AnyTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, None, list, tuple, bytes],
                        ) -> 'items':
                            return super().__new__(
                                cls,
                                *_args,
                                _configuration=_configuration,
                                **kwargs,
                            )
            
                def __new__(
                    cls,
                    _arg: typing.Union[typing.Tuple[typing.Union[MetaOapg.items, dict, frozendict.frozendict, ]], typing.List[typing.Union[MetaOapg.items, dict, frozendict.frozendict, ]]],
                    _configuration: typing.Optional[schemas.Configuration] = None,
                ) -> 'charFilters':
                    return super().__new__(
                        cls,
                        _arg,
                        _configuration=_configuration,
                    )
            
                def __getitem__(self, i: int) -> MetaOapg.items:
                    return super().__getitem__(i)
            
            
            class tokenFilters(
                schemas.ListSchema
            ):
            
            
                class MetaOapg:
                    
                    
                    class items(
                        schemas.ComposedSchema,
                    ):
                    
                    
                        class MetaOapg:
                            
                            @classmethod
                            @functools.lru_cache()
                            def any_of(cls):
                                # we need this here to make our import statements work
                                # we must store _composed_schemas in here so the code is only run
                                # when we invoke this method. If we kept this at the class
                                # level we would get an error because the class level
                                # code would be run when this module is imported, and these composed
                                # classes don't exist yet because their module has not finished
                                # loading
                                return [
                                    TokenFilterasciiFolding,
                                    TokenFilterdaitchMokotoffSoundex,
                                    TokenFilteredgeGram,
                                    TokenFiltericuFolding,
                                    TokenFiltericuNormalizer,
                                    TokenFilterlength,
                                    TokenFilterlowercase,
                                    TokenFilternGram,
                                    TokenFilterregex,
                                    TokenFilterreverse,
                                    TokenFiltershingle,
                                    TokenFiltersnowballStemming,
                                    TokenFilterstopword,
                                    TokenFiltertrim,
                                ]
                    
                    
                        def __new__(
                            cls,
                            *_args: typing.Union[dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
                            _configuration: typing.Optional[schemas.Configuration] = None,
                            **kwargs: typing.Union[schemas.AnyTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, None, list, tuple, bytes],
                        ) -> 'items':
                            return super().__new__(
                                cls,
                                *_args,
                                _configuration=_configuration,
                                **kwargs,
                            )
            
                def __new__(
                    cls,
                    _arg: typing.Union[typing.Tuple[typing.Union[MetaOapg.items, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ]], typing.List[typing.Union[MetaOapg.items, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ]]],
                    _configuration: typing.Optional[schemas.Configuration] = None,
                ) -> 'tokenFilters':
                    return super().__new__(
                        cls,
                        _arg,
                        _configuration=_configuration,
                    )
            
                def __getitem__(self, i: int) -> MetaOapg.items:
                    return super().__getitem__(i)
            __annotations__ = {
                "name": name,
                "tokenizer": tokenizer,
                "charFilters": charFilters,
                "tokenFilters": tokenFilters,
            }
    
    name: MetaOapg.properties.name
    tokenizer: MetaOapg.properties.tokenizer
    
    @typing.overload
    def __getitem__(self, name: typing_extensions.Literal["name"]) -> MetaOapg.properties.name: ...
    
    @typing.overload
    def __getitem__(self, name: typing_extensions.Literal["tokenizer"]) -> MetaOapg.properties.tokenizer: ...
    
    @typing.overload
    def __getitem__(self, name: typing_extensions.Literal["charFilters"]) -> MetaOapg.properties.charFilters: ...
    
    @typing.overload
    def __getitem__(self, name: typing_extensions.Literal["tokenFilters"]) -> MetaOapg.properties.tokenFilters: ...
    
    @typing.overload
    def __getitem__(self, name: str) -> schemas.UnsetAnyTypeSchema: ...
    
    def __getitem__(self, name: typing.Union[typing_extensions.Literal["name", "tokenizer", "charFilters", "tokenFilters", ], str]):
        # dict_instance[name] accessor
        return super().__getitem__(name)
    
    
    @typing.overload
    def get_item_oapg(self, name: typing_extensions.Literal["name"]) -> MetaOapg.properties.name: ...
    
    @typing.overload
    def get_item_oapg(self, name: typing_extensions.Literal["tokenizer"]) -> MetaOapg.properties.tokenizer: ...
    
    @typing.overload
    def get_item_oapg(self, name: typing_extensions.Literal["charFilters"]) -> typing.Union[MetaOapg.properties.charFilters, schemas.Unset]: ...
    
    @typing.overload
    def get_item_oapg(self, name: typing_extensions.Literal["tokenFilters"]) -> typing.Union[MetaOapg.properties.tokenFilters, schemas.Unset]: ...
    
    @typing.overload
    def get_item_oapg(self, name: str) -> typing.Union[schemas.UnsetAnyTypeSchema, schemas.Unset]: ...
    
    def get_item_oapg(self, name: typing.Union[typing_extensions.Literal["name", "tokenizer", "charFilters", "tokenFilters", ], str]):
        return super().get_item_oapg(name)
    

    def __new__(
        cls,
        *_args: typing.Union[dict, frozendict.frozendict, ],
        name: typing.Union[MetaOapg.properties.name, str, ],
        tokenizer: typing.Union[MetaOapg.properties.tokenizer, dict, frozendict.frozendict, ],
        charFilters: typing.Union[MetaOapg.properties.charFilters, list, tuple, schemas.Unset] = schemas.unset,
        tokenFilters: typing.Union[MetaOapg.properties.tokenFilters, list, tuple, schemas.Unset] = schemas.unset,
        _configuration: typing.Optional[schemas.Configuration] = None,
        **kwargs: typing.Union[schemas.AnyTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, None, list, tuple, bytes],
    ) -> 'ApiAtlasFTSAnalyzersViewManual':
        return super().__new__(
            cls,
            *_args,
            name=name,
            tokenizer=tokenizer,
            charFilters=charFilters,
            tokenFilters=tokenFilters,
            _configuration=_configuration,
            **kwargs,
        )

from atlas.model.char_filterhtml_strip import CharFilterhtmlStrip
from atlas.model.char_filtericu_normalize import CharFiltericuNormalize
from atlas.model.char_filtermapping import CharFiltermapping
from atlas.model.char_filterpersian import CharFilterpersian
from atlas.model.token_filterascii_folding import TokenFilterasciiFolding
from atlas.model.token_filterdaitch_mokotoff_soundex import TokenFilterdaitchMokotoffSoundex
from atlas.model.token_filteredge_gram import TokenFilteredgeGram
from atlas.model.token_filtericu_folding import TokenFiltericuFolding
from atlas.model.token_filtericu_normalizer import TokenFiltericuNormalizer
from atlas.model.token_filterlength import TokenFilterlength
from atlas.model.token_filterlowercase import TokenFilterlowercase
from atlas.model.token_filtern_gram import TokenFilternGram
from atlas.model.token_filterregex import TokenFilterregex
from atlas.model.token_filterreverse import TokenFilterreverse
from atlas.model.token_filtershingle import TokenFiltershingle
from atlas.model.token_filtersnowball_stemming import TokenFiltersnowballStemming
from atlas.model.token_filterstopword import TokenFilterstopword
from atlas.model.token_filtertrim import TokenFiltertrim
from atlas.model.tokenizeredge_gram import TokenizeredgeGram
from atlas.model.tokenizerkeyword import Tokenizerkeyword
from atlas.model.tokenizern_gram import TokenizernGram
from atlas.model.tokenizerregex_capture_group import TokenizerregexCaptureGroup
from atlas.model.tokenizerregex_split import TokenizerregexSplit
from atlas.model.tokenizerstandard import Tokenizerstandard
from atlas.model.tokenizeruax_url_email import TokenizeruaxUrlEmail
from atlas.model.tokenizerwhitespace import Tokenizerwhitespace
